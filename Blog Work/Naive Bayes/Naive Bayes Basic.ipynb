{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read sample messages dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mails_df = pd.read_csv('msgs.csv')\n",
    "mails_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nltk word_tokenize to convert the message strings into list of words\n",
    "\n",
    "using stopwords to remove punctuations or irrelevant words from list of words\n",
    "\n",
    "using PorterStemmer to convert the words into their root form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def process_message(message):\n",
    "    \n",
    "    message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    \n",
    "    sw = stopwords.words('english')\n",
    "    words = [word for word in words if word not in sw]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]  \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mails_df['clean_msg'] = mails_df['msg'].apply(process_message)\n",
    "mails_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a word_frequency.json file and start adding each word under ham or spam category in json file with initial count of 1 for new word in file. With repetition of same word, we can increase the count of repititve word under ham or spam caegory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def word_frequency_category(df):\n",
    "    cls = df[1]\n",
    "    json_file = open('word_frequency.json')\n",
    "    \n",
    "    word_freq = json.load(json_file)\n",
    "    for word in df['clean_msg']:\n",
    "        if word in word_freq['class'][cls]:\n",
    "            word_freq['class'][cls][word] += 1\n",
    "        else:\n",
    "            word_freq['class'][cls][word] = 1\n",
    "            \n",
    "    json_file = open('word_frequency.json', 'w')\n",
    "    json.dump(word_freq, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails_df[['clean_msg', 'class']].apply(word_frequency_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mails_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p(ham|msg text) = ( p(ham) * p(msg text|ham) ) / p(msg text)\n",
    "\n",
    "## p(spam|txt) = ( p(spam) * p(msg text|spam) ) / p(msg text)\n",
    "\n",
    "Since P(msg text) is constant and common in both expressions, we can avoid it. Note that our goal is not to calculate the actual probability but only comparison. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P(ham) = no of documents belonging to category ham / total no of documents\n",
    "\n",
    "## P(spam) = no of documents belonging to category spam / total no of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_ham = mails_df[mails_df['class']=='ham'].shape[0]/ mails_df.shape[0]\n",
    "p_spam = mails_df[mails_df['class']=='spam'].shape[0]/ mails_df.shape[0]\n",
    "p_ham, p_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the above two probabilities, we’ll use wordFrequency file.\n",
    "\n",
    "P(bodyText | spam) = P(word1 | spam) * P(word2 | spam) * …\n",
    "\n",
    "P(bodyText | ham) = P(word1 | ham) * P(word2 | ham) * …\n",
    "\n",
    "\n",
    "To calculate the above two probabilities, we’ll use wordFrequency file. Here word1, word2, word3 up to word-n are total words in bodytext. \n",
    "\n",
    "P(word1 | spam) = count of word1 belonging to category spam / total count of words belonging to category spam.\n",
    "\n",
    "P(word1 | ham) = count of word1 belonging to category ham / total count of words belonging to category ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = json.load(open('word_frequency.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words_count = len(word_freq['class']['spam'])\n",
    "ham_words_count = len(word_freq['class']['ham'])\n",
    "spam_words_count, ham_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ham_spam_msg_prob(msg):\n",
    "    words = process_message(msg)\n",
    "    \n",
    "    ham_prob = 1\n",
    "    spam_prob = 1\n",
    "    \n",
    "    ham_counter=0\n",
    "    spam_couter=0\n",
    "#     print(words)\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        if word in word_freq['class']['ham']:\n",
    "            print('ham prob: ', ((word_freq['class']['ham'][word])/ham_words_count))\n",
    "            ham_prob *= ((word_freq['class']['ham'][word])/ham_words_count)\n",
    "            ham_counter += 1\n",
    "            print('Ham: ', ham_prob)\n",
    "            \n",
    "        if word in word_freq['class']['spam']:\n",
    "            print('spam_prob: ',((word_freq['class']['spam'][word])/spam_words_count))\n",
    "            spam_prob *= ((word_freq['class']['spam'][word])/spam_words_count)\n",
    "            spam_couter += 1\n",
    "            print('Spam: ', spam_prob)\n",
    "    \n",
    "    ham_prob = p_ham * ham_prob\n",
    "    spam_prob = p_spam * spam_prob\n",
    "    \n",
    "#     print(ham_prob, spam_prob)\n",
    "    print(ham_counter, spam_couter)\n",
    "        \n",
    "    if ham_counter == 0 and spam_couter == 0:\n",
    "        print('Cant clasify')\n",
    "    elif ham_counter == 0 and spam_couter != 0:\n",
    "        print('Its spam')\n",
    "        return ham_prob, spam_prob\n",
    "    elif ham_counter != 0 and spam_couter == 0:\n",
    "        print('Its not spam')\n",
    "        return ham_prob, spam_prob\n",
    "    elif ham_prob > spam_prob:\n",
    "        print('Its not spam')\n",
    "    elif ham_prob < spam_prob:\n",
    "        print('Its spam')\n",
    "    else:\n",
    "        print('Cant clasify')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_spam_msg_prob('Free Win cash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
